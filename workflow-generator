#!/usr/bin/env python

from AutoADAG import *
import ConfigParser
from Pegasus.DAX3 import *
import getpass
import logging
import math
import optparse
import os
import re
import socket
import string
import subprocess
import sys
import time



# to setup python lib dir for importing Pegasus PYTHON DAX API
pegasus_config = os.path.join("pegasus-config") + " --noeoln --python"
lib_dir = subprocess.Popen(pegasus_config,
                           stdout=subprocess.PIPE,
                           shell=True).communicate()[0]
#Insert this directory in our search path
os.sys.path.insert(0, lib_dir)


# --- global variables ----------------------------------------------------------------

logger      = logging.getLogger("my_logger")
conf        = None
added_execs = []


# --- classes -------------------------------------------------------------------------

class ComputeJob(Job):
    """ A Pegasus DAX Job with extra information such as cpu and memory
    requirements, for both single and peagaus-mpi-cluster execution
    """

    def __init__(self, name, cores=1, mem_gb=2, partition="part1"):
        Job.__init__(self, name=name)
        
        # label based clustering
        self.addProfile(Profile(Namespace.PEGASUS, 
                                key="label",
                                value=partition))
  
        # standard resource requirements for all jobs
        mem_mb = mem_gb * 1000
        self.addProfile(Profile(Namespace.CONDOR,
                                key="request_cpus",
                                value=str(cores)))
        self.addProfile(Profile(Namespace.PEGASUS,
                                key="pmc_request_cpus",
                                value=str(cores)))
        self.addProfile(Profile(Namespace.CONDOR,
                                key="request_memory",
                                value=str(mem_mb)))
        self.addProfile(Profile(Namespace.PEGASUS,
                                key="pmc_request_memory",
                                value=str(mem_mb)))
        self.addProfile(Profile(Namespace.CONDOR,
                                key="request_disk",
                                value=str(20*1024*1024)))

        # special sauce for TACC - we want smaller jobs to go to the normal
        # compute nodes and the large memory ones to go to the large memory
        # nodes
        if re.search('stampede', conf.get("local", "exec_env")):
            hosts = conf.get("exec_environment", "hosts_" + partition)
            cores = str(16 * int(hosts))
            self.addProfile(Profile(Namespace.GLOBUS,
                                    key="queue",
                                    value="normal"))
            self.addProfile(Profile(Namespace.GLOBUS,
                                    key="hostcount",
                                    value=hosts))
            self.addProfile(Profile(Namespace.GLOBUS,
                                    key="count",
                                    value=cores))
            self.addProfile(Profile(Namespace.ENV,
                                    key="PMC_HOST_MEMORY",
                                    value="29000"))
        
        # required for the Pegasus accounting
        self.addProfile(Profile(Namespace.PEGASUS,
                                key="cores",
                                value=str(cores)))
  


# --- functions -----------------------------------------------------------------------


def setup_logger(verbose):
    """ Use a console logger for all output to the user """

    # log to the console
    console = logging.StreamHandler()

    # default log level - make logger/console match
    logger.setLevel(logging.INFO)
    console.setLevel(logging.INFO)

    if verbose:
        logger.setLevel(logging.DEBUG)
        console.setLevel(logging.DEBUG)

    # formatter
    formatter = logging.Formatter("%(asctime)s %(levelname)7s:  %(message)s")
    console.setFormatter(formatter)
    logger.addHandler(console)
    logger.debug("Logger has been configured")


def myexec(cmd_line):
    """ Convenience function as we are shelling out a fair amount """
    
    sys.stdout.flush()
    p = subprocess.Popen(cmd_line + " 2>&1", shell=True)
    stdoutdata, stderrdata = p.communicate()
    r = p.returncode
    if r != 0:
        raise RuntimeError("Command '%s' failed with error code %s" \
                           % (cmd_line, r))


def proxy_check():
    """ Verify that the user has a proxy and it is valid for a long time """
    p = subprocess.Popen("grid-proxy-info -timeleft", shell=True, 
                         stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    stdoutdata, stderrdata = p.communicate()
    r = p.returncode
    if r != 0:
        logger.error(stderrdata)
        raise RuntimeError("Unable to run the grid-proxy-info command." + \
                           "Do you have a valid proxy?")
    timeleft = int(stdoutdata)

    # two weeks minimum lifetime
    if timeleft < (60*60*24*10):
        raise RuntimeError("There is not enough time left on your grid" +
                           " proxy. Please renew, then run this command" +
                           " again")

        
def generate_site_catalog():
    """ Uses a templete file to produce the Pegasus site catalog """
    
    logger.info("Generating sites.catalog")
    inf = open("conf/" + conf.get("local", "exec_env") + 
               "/sites.catalog.template", 'r')
    template = string.Template(inf.read())
    inf.close()

    outf = open(conf.get("local", "work_dir") + "/sites.catalog", "w")
    outf.write(template.substitute(
                        submit_host = socket.gethostname(),
                        username = getpass.getuser(), 
                        home = os.path.expanduser('~'),
                        top_dir = conf.get("local", "top_dir"),
                        work_dir = conf.get("local", "work_dir"),
                        pegasus_bin = conf.get("local", "pegasus_bin"),
                        irods_bin = conf.get("local", "irods_bin"),
                        tacc_allocation = conf.get("tacc", "allocation"),
                        tacc_username = conf.get("tacc", "username"),
                        tacc_storage_group = conf.get("tacc", "storage_group"),
              ))
    outf.close()
   

def read_input_lists(ref_urls, chromosomes, fastq_urls):
    """ The user provides a list of reference file URLs and pairs of fastq 
    URLs to be processed.
    """
    
    # first the reference
    inf = open(conf.get("local", "top_dir") + "/inputs-ref.txt", "r")
    for line in inf.readlines():
        line = line.rstrip('\n')
        if len(line) > 0:
            ref_urls.append(line)
    inf.close()

    # chromosomes
    inf = open(conf.get("local", "top_dir") + "/chromosomes.txt", "r")
    for line in inf.readlines():
        line = line.rstrip('\n')
        line = re.sub("^> *", "", line)
        line = re.sub(" .*", "", line)
        if len(line) > 0:
            logger.info("  Added chromosome: " + line)
            chromosomes.append(line)
    inf.close()
    
    inf = open(conf.get("local", "top_dir") + "/inputs-fastq.txt", "r")
    for line in inf.readlines():
        line = line.rstrip('\n')
        if len(line) > 0:
            fastq_urls.append(line)
    inf.close()

    # sanity checks
    if len(ref_urls) != 1:
        logger.error("Only one reference genome can be listed in inputs-ref.txt");
        sys.exit(1) 
    if len(fastq_urls) == 0:
        logger.error("Did not find fastq files")
        sys.exit(1)
    if conf.get("main", "inputs-style") == "double-end" and len(fastq_urls) % 2 != 0:
        logger.error("Found an uneven number of fastq files in input list")
        sys.exit(1)


def extract_lfn(url):
    """ determine a logical file name (basename) from a given URL """
    return re.sub(".*/", "", url)


def local_pfn(path):
    """ generates a full pfn given a local path """
    pfn = "scp://" + getpass.getuser() + "@" + socket.gethostname() + "/" + path
    return pfn
  

def extract_fasta_basefile(file_list):
    """ find the base fasta file given a list of reference files """
    for f in file_list:
        if re.search("(.fa|.fasta)$", f.name):
            return f


def extract_sample_name(url):
    """ sample name is the fist part of the base name (for example:
    HN0001 from HN001_FCD1P1JACXX_L6_SZAIPI024836-36_1.fq)
    """
    name = re.sub(".*/", "", url)
    name = re.sub("_.*", "", name)
    name = re.sub("\..*", "", name)
    return name


def add_executable(dax, logical_name, wrapper_name):
    """ adds executables to the DAX-level replica catalog """
    global added_execs

    if logical_name in added_execs:
        return

    base_url = local_pfn(conf.get("local", "top_dir"))    
    
    wrapper = Executable(name=logical_name, 
                         arch="x86_64",
                         installed=False)
    wrapper.addPFN(PFN(base_url + "/wrappers/" + wrapper_name, "local"))
    dax.addExecutable(wrapper)

    added_execs.append(logical_name)


def gunzip_job(dax, software_tar, f_in, f_out, parent_jobs=None):
    """ adds a job to gunzip an input file
    """
    add_executable(dax, "gunzip", "gunzip-wrapper")
    j = ComputeJob("gunzip", cores = 1, mem_gb = 2, partition = "part1")
    j.uses(software_tar, link=Link.INPUT)
    j.uses(f_in, link=Link.INPUT)
    j.uses(f_out, link=Link.OUTPUT, transfer=False)
    j.addArguments(f_in, f_out)
    dax.addJob(j)
    if parent_jobs is not None:
        for parent in parent_jobs:
            dax.depends(parent=parent, child=j)


def prepare_ref_genome(dax, software_job, software_tar, ref_url, ref_files):
    
    # temp list of files we need to keep track of
    files = {}
   
    lfn = extract_lfn(ref_url)
    lfn_base = re.sub("\.[a-zA-Z0-9]+$", "", lfn)
    lfn_ext = re.sub(".*\.", "", lfn)

    j1 = None
    if lfn_ext == "gz":
        # add a job to gunzip the input
        in_f = File(lfn)
        in_f.addPFN(PFN(ref_url, "irods_iplant"))
        dax.addFile(in_f)

        fa_f = File(lfn_base + ".fa")
        gunzip_job(dax, software_tar, in_f, fa_f, [software_job])

        # update the lfn for subsequent jobs
        lfn = lfn_base + ".fa"
        lfn_ext = "fa"
    elif lfn_ext == "fa":
        # already a .fa file
        fa_f = File(lfn)
        fa_f.addPFN(PFN(ref_url, "irods_iplant"))
        dax.addFile(in_f)
    else:
        logger.error("Unable to handle reference genome with %s extension" %(lfn_ext))
        sys.exit(1)
    files[fa_f.name] = fa_f
   
    # bwa index
    add_executable(dax, "bwa-index", "bwa-wrapper")
    j2 = ComputeJob("bwa-index", cores = 1, mem_gb = 4, partition = "part1")
    j2.uses(software_tar, link=Link.INPUT)
    j2.uses(fa_f, link=Link.INPUT)
    for ext in [".amb", ".ann", ".bwt", ".pac", ".sa"]:
        f = File(lfn + ext)
        files[f.name] = f
        j2.uses(f, link=Link.OUTPUT, transfer=False)
    j2.addArguments("index", fa_f)
    dax.addJob(j2)
    if not j1:
        dax.depends(parent=software_job, child=j2)
    
    # samtools faidx
    add_executable(dax, "faidx", "samtools-wrapper")
    j3 = ComputeJob("faidx", cores = 1, mem_gb = 4, partition = "part1")
    j3.uses(software_tar, link=Link.INPUT)
    j3.uses(fa_f, link=Link.INPUT)
    f = File(lfn_base + ".fa.fai")
    files[f.name] = f
    j3.uses(f, link=Link.OUTPUT, transfer=False)
    j3.addArguments("faidx", fa_f)
    dax.addJob(j3)

    # picard sequence dictionary
    add_executable(dax, "seq_dict", "picard-wrapper")
    j4 = ComputeJob("seq_dict", cores = 1, mem_gb = 4, partition = "part1")
    j4.uses(software_tar, link=Link.INPUT)
    j4.uses(fa_f, link=Link.INPUT)
    f = File(lfn_base + ".dict")
    files[f.name] = f
    j4.uses(f, link=Link.OUTPUT, transfer=False)
    j4.addArguments("CreateSequenceDictionary.jar", 
                    "REFERENCE=" + fa_f.name,
                    "OUTPUT=" + f.name)
    dax.addJob(j4)

    # populate the reference file list which the rest of the workflow needs
    for key, f in files.iteritems():
        ref_files.append(f)


def alignment_to_reference(sample_name, dax, software_tar, chromosomes, ref_files, tracked_files):
    
    # Note that the cores we give Pegasus and the -t does not match. 
    # Oversubscriptions is ok, as bwa can not keep all the cores busy 100%
    # of the time.
    add_executable(dax, "alignment_to_reference", "bwa-wrapper")
    j = ComputeJob("alignment_to_reference", cores = 7, mem_gb = 8,
                   partition = "part1")
    
    # determine which is the fasta file
    for f in ref_files:
        j.uses(f, link=Link.INPUT)
    j.uses(software_tar, link=Link.INPUT)
    j.uses(tracked_files['sam'], link=Link.OUTPUT, transfer=False)
    j.setStdout(tracked_files['sam'])
        
    if conf.get("main", "inputs-style") == "single-end":
        # single-end inputs 
        j.uses(tracked_files['fastq_input'], link=Link.INPUT)
        j.addArguments("mem",
                       "-t", "12",
                       "-M", extract_fasta_basefile(ref_files), 
                       tracked_files['fastq_input'])
    else:
        # pair-end inputs
        j.uses(tracked_files['paired_read1_fastq'], link=Link.INPUT)
        j.uses(tracked_files['paired_read2_fastq'], link=Link.INPUT)
        j.addArguments("mem",
                       "-t", "12",
                       "-M", extract_fasta_basefile(ref_files), 
                       tracked_files['paired_read1_fastq'],
                       tracked_files['paired_read2_fastq'])
    dax.addJob(j)

    # next step    
    sortsam_job = sort_sam(sample_name, dax, software_tar, chromosomes, ref_files, tracked_files)
    
    return j


def sort_sam(sample_name, dax, software_tar, chromosomes, ref_files, tracked_files):

    add_executable(dax, "sort_sam", "picard-wrapper")
    j = ComputeJob("sort_sam", cores = 1, mem_gb = 21, 
                   partition = "part1")
    j.uses(software_tar, link=Link.INPUT)
    j.uses(tracked_files['sam'], link=Link.INPUT)
    j.uses(tracked_files['sorted_reads'], link=Link.OUTPUT, transfer=False)
    j.uses(tracked_files['sorted_index'], link=Link.OUTPUT, transfer=False)
    
    j.addArguments("SortSam.jar",
                   "CREATE_INDEX=TRUE",
                   "MAX_RECORDS_IN_RAM=5000000",
                   "I=" + tracked_files['sam'].name,
                   "O=" + tracked_files['sorted_reads'].name,
                   "SO=coordinate",
                   "VALIDATION_STRINGENCY=LENIENT")
    dax.addJob(j)
    
    dedup_job = dedup(sample_name, dax, software_tar, chromosomes, ref_files, tracked_files)

    

def dedup(sample_name, dax, software_tar, chromosomes, ref_files, tracked_files):
    add_executable(dax, "dedup", "picard-wrapper")
    j = ComputeJob("dedup", cores = 1, mem_gb = 21,
                   partition = "part1")
    j.uses(software_tar, link=Link.INPUT)
    j.uses(tracked_files['sorted_reads'], link=Link.INPUT)
    j.uses(tracked_files['sorted_index'], link=Link.INPUT)
    j.uses(tracked_files['deduped_reads'], link=Link.OUTPUT, transfer=False)
    j.uses(tracked_files['deduped_index'], link=Link.OUTPUT, transfer=False)
    #j.uses(tracked_files['deduped_metrics'], link=Link.OUTPUT, transfer=False)
    
    j.addArguments("MarkDuplicates.jar",
                   "CREATE_INDEX=TRUE",
                   "MAX_RECORDS_IN_RAM=5000000",
                   "I=" + tracked_files['sorted_reads'].name,
                   "O=" + tracked_files['deduped_reads'].name,
                   "METRICS_FILE=" + tracked_files['deduped_metrics'].name,
                   "VALIDATION_STRINGENCY=LENIENT")

    dax.addJob(j)

    add_replace(sample_name, dax, software_tar, chromosomes, ref_files, tracked_files)


def add_replace(sample_name, dax, software_tar, chromosomes, ref_files, tracked_files):
    add_executable(dax, "add_replace", "picard-wrapper")
    j = ComputeJob("add_replace", cores = 1, mem_gb = 21,
                   partition = "part1")
    j.uses(software_tar, link=Link.INPUT)
    j.uses(tracked_files['deduped_reads'], link=Link.INPUT)
    j.uses(tracked_files['deduped_index'], link=Link.INPUT)
    j.uses(tracked_files['addrepl_reads'], link=Link.OUTPUT, transfer=True)
    j.uses(tracked_files['addrepl_index'], link=Link.OUTPUT, transfer=True)
    
    j.addArguments("AddOrReplaceReadGroups.jar",
                   "MAX_RECORDS_IN_RAM=5000000",
                   "I=" + tracked_files['deduped_reads'].name,
                   "O=" + tracked_files['addrepl_reads'].name,
                   "RGID=" + sample_name,
                   "LB=" + sample_name,
                   "PL=\"Illumina\"",
                   "SM=" + sample_name,
                   "CN=BGI",
                   "RGPU=" + sample_name,
                   "VALIDATION_STRINGENCY=LENIENT",
                   "SORT_ORDER=coordinate",
                   "CREATE_INDEX=TRUE")
    dax.addJob(j)
    
    realign_target_creator(sample_name, dax, software_tar, chromosomes, ref_files, tracked_files)


def realign_target_creator(sample_name, dax, software_tar, chromosomes, ref_files, tracked_files):
    add_executable(dax, "realign_target_creator", "gatk-wrapper")
    j = ComputeJob("realign_target_creator", cores = 15, mem_gb = 10,
                   partition = "part1")
    j.uses(software_tar, link=Link.INPUT)
    for f in ref_files:
        j.uses(f, link=Link.INPUT)
    j.uses(tracked_files['addrepl_reads'], link=Link.INPUT)
    j.uses(tracked_files['addrepl_index'], link=Link.INPUT)
    j.uses(tracked_files['intervals'], link=Link.OUTPUT, transfer=False)
    
    j.addArguments("-T", "RealignerTargetCreator",
                   "-nt", "15",
                   "-R", extract_fasta_basefile(ref_files),
                   "-I", tracked_files['addrepl_reads'],
                   "-o", tracked_files['intervals']) 
    dax.addJob(j)
    
    indel_realign(sample_name, dax, software_tar, chromosomes, ref_files, tracked_files)
    

def indel_realign(sample_name, dax, software_tar, chromosomes, ref_files, tracked_files):
    # IndelRealigner can only be run single threaded
    add_executable(dax, "indel_realign", "gatk-wrapper")   
    j = ComputeJob("indel_realign", cores = 1, mem_gb = 10,
                   partition = "part1")
    j.uses(software_tar, link=Link.INPUT)
    for f in ref_files:
        j.uses(f, link=Link.INPUT)
    j.uses(tracked_files['addrepl_reads'], link=Link.INPUT)
    j.uses(tracked_files['addrepl_index'], link=Link.INPUT)
    j.uses(tracked_files['intervals'], link=Link.INPUT)
    j.uses(tracked_files['indel_realigned_reads'], link=Link.OUTPUT, transfer=True)
    j.uses(tracked_files['indel_realigned_index'], link=Link.OUTPUT, transfer=True)
    
    j.addArguments("-T", "IndelRealigner",
                   "-R", extract_fasta_basefile(ref_files),
                   "-I", tracked_files['addrepl_reads'],
                   "-targetIntervals", tracked_files['intervals'],
                   "-o", tracked_files['indel_realigned_reads'])
    dax.addJob(j)
    
    for chr in chromosomes:
        haplotype_caller(sample_name, dax, software_tar,
                         ref_files, tracked_files, chr)


def select_and_filter_snp(dax, software_file, ref_files, tracked_files,
                          in_file, out_file, out_idx):
    
    # we need an intermediate file
    intername = re.sub(".*/", "", in_file.name) + "_snp_only.vcf"
    tracked_files[intername] = File(intername) 
    
    add_executable(dax, "select_variants_snp", "gatk-wrapper")    
    j = ComputeJob("select_variants_snp", cores = 14, mem_gb = 10,
                   partition = "part3")
    
    # inputs
    j.uses(software_file, link=Link.INPUT)
    for f in ref_files:
        j.uses(f, link=Link.INPUT)
    j.uses(in_file, link=Link.INPUT)

    # outputs
    j.uses(tracked_files[intername], link=Link.OUTPUT, transfer=False)
    
    j.addArguments("-T", "SelectVariants",
                   "-nt", "15",
                   "-R", extract_fasta_basefile(ref_files),
                   "-selectType", "SNP",
                   "-V", in_file,
                   "-o", tracked_files[intername])
    
    dax.addJob(j)
    
    add_executable(dax, "filtering_snp", "gatk-wrapper")    
    j = ComputeJob("filtering_snp", cores = 1, mem_gb = 10,
                   partition = "part3")
    
    # inputs
    j.uses(software_file, link=Link.INPUT)
    for f in ref_files:
        j.uses(f, link=Link.INPUT)
    j.uses(tracked_files[intername], link=Link.INPUT)

    # outputs
    j.uses(out_file, link=Link.OUTPUT, transfer=True)
    j.uses(out_idx, link=Link.OUTPUT, transfer=True)
    
    j.addArguments("-T", "VariantFiltration",
                   "-R", extract_fasta_basefile(ref_files),
                   "-V", tracked_files[intername],
                   "--filterExpression", "'" + conf.get("main", "snp_filter") + "'",
                   "--filterName", "my_snp_filter",
                   "-o", out_file)

    dax.addJob(j)


def select_and_filter_indel(dax, software_file, ref_files, tracked_files,
                            in_file, out_file, out_idx):

    # we need an intermediate file
    intername = re.sub(".*/", "", in_file.name) + "_indel_only.vcf"
    tracked_files[intername] = File(intername) 

        
    add_executable(dax, "select_variants_indel", "gatk-wrapper")    
    j = ComputeJob("select_variants_indel", cores = 14, mem_gb = 10,
                   partition = "part3")
    
    # inputs
    j.uses(software_file, link=Link.INPUT)
    for f in ref_files:
        j.uses(f, link=Link.INPUT)
    j.uses(in_file, link=Link.INPUT)

    # outputs
    j.uses(tracked_files[intername], link=Link.OUTPUT, transfer=False)
    
    j.addArguments("-T", "SelectVariants",
                   "-nt", "15",
                   "-R", extract_fasta_basefile(ref_files),
                   "-selectType", "INDEL",
                   "-V", in_file,
                   "-o", tracked_files[intername])
 
    dax.addJob(j)
 
    add_executable(dax, "filtering_indel", "gatk-wrapper")    
    j = ComputeJob("filtering_indel", cores = 1, mem_gb = 10,
                   partition = "part3")
    
    # inputs
    j.uses(software_file, link=Link.INPUT)
    for f in ref_files:
        j.uses(f, link=Link.INPUT)
    j.uses(tracked_files[intername], link=Link.INPUT)

    # outputs
    j.uses(out_file, link=Link.OUTPUT, transfer=True)
    j.uses(out_idx, link=Link.OUTPUT, transfer=True)
    
    j.addArguments("-T", "VariantFiltration",
                   "-R", extract_fasta_basefile(ref_files),
                   "-V", tracked_files[intername],
                   "--filterExpression", "'" + conf.get("main", "indel_filter") + "'",
                   "--filterName", "my_indel_filter",
                   "-o", out_file)
 
    dax.addJob(j)


def haplotype_caller(sample_name, dax, software_file, ref_files, tracked_files,
                     chromosome):

    add_executable(dax, "haplotype_caller", "gatk-wrapper")
    j = ComputeJob("haplotype_caller", cores = 1, mem_gb = 5,
                   partition = "part2")
    
    # inputs
    j.uses(software_file, link=Link.INPUT)
    for f in ref_files:
        j.uses(f, link=Link.INPUT)
    j.uses(tracked_files['indel_realigned_reads'], link=Link.INPUT)
    j.uses(tracked_files['indel_realigned_index'], link=Link.INPUT)

    # outputs
    fname = conf.get("local", "run_id") + "-" + sample_name + "_" + chromosome + ".vcf"
    tracked_files[fname] = File(fname)
    j.uses(tracked_files[fname], link=Link.OUTPUT, transfer=False)
    tracked_files[fname + ".idx"] = File(fname + ".idx")
    j.uses(tracked_files[fname + ".idx"], link=Link.OUTPUT, transfer=False)
    
    j.addArguments("-T", "HaplotypeCaller",
                   "--emitRefConfidence", "GVCF",
                   "--variant_index_type", "LINEAR",
                   "--variant_index_parameter", "128000",
                   "-L", chromosome,
                   "-R", extract_fasta_basefile(ref_files),
                   "-I", tracked_files['indel_realigned_reads'],
                   "-o", tracked_files[fname])
    dax.addJob(j)


def merge_gvcf(dax, software_file, chromosomes, ref_files, tracked_files, sample_names):

    add_executable(dax, "merge_gcvf", "gatk-wrapper")
    j = ComputeJob("merge_gcvf", cores = 1, mem_gb = 20,
                   partition = "part3")
    
    # inputs
    files = []
    j.uses(software_file, link=Link.INPUT)
    for f in ref_files:
        j.uses(f, link=Link.INPUT)
    for s in sample_names:
        for chr in chromosomes:
            fname = "%s-%s_%s.vcf" % (conf.get("local", "run_id"), s, chr)
            j.uses(tracked_files[fname + ".idx"], link=Link.INPUT)
            j.uses(tracked_files[fname], link=Link.INPUT)
            files.append(fname)

    # create filelist to minimize the length of the command line
    fd = open(conf.get("local", "work_dir") + "/haplotype-files.list", "w")
    for f in files:
        fd.write("%s\n" %(f))
    fd.close()
    hf = File("haplotype-files.list")
    hf.addPFN(PFN(local_pfn(conf.get("local", "work_dir") + "/haplotype-files.list"), "local"))
    dax.addFile(hf)
    j.uses("haplotype-files.list", link=Link.INPUT)

    # outputs
    fname = conf.get("local", "run_id") + "-mergeGVCF.vcf"
    tracked_files[fname] = File(fname)
    j.uses(tracked_files[fname], link=Link.OUTPUT, transfer=True)
    tracked_files[fname + ".idx"] = File(fname + ".idx")
    j.uses(tracked_files[fname + ".idx"], link=Link.OUTPUT, transfer=True)
    
    j.addArguments("-T", "CombineGVCFs",
                   "-R", extract_fasta_basefile(ref_files),
                   "-o", tracked_files[fname],
                   "--variant", "haplotype-files.list")
    
    dax.addJob(j)


def genotype_gvcfs(dax, software_file, ref_files, tracked_files, sample_names,
                   chromosome):

    add_executable(dax, "genotype_gvcfs", "gatk-wrapper")
    j = ComputeJob("genotype_gvcfs", cores = 1, mem_gb = 10,
                   partition = "part2")
    
    # inputs
    variant_files = []
    j.uses(software_file, link=Link.INPUT)
    for f in ref_files:
        j.uses(f, link=Link.INPUT)
    for sname in sample_names:
        fname = conf.get("local", "run_id") + "-" + sname + "_" + chromosome + ".vcf"
        f = tracked_files[fname]
        j.uses(f, link=Link.INPUT)
        j.uses(tracked_files[fname + ".idx"], link=Link.INPUT)
        variant_files.append(f)

    # outputs
    fname = conf.get("local", "run_id") + "-" + "GVCF_" + chromosome + ".vcf"
    tracked_files[fname] = File(fname)
    j.uses(tracked_files[fname], link=Link.OUTPUT, transfer=False)
    tracked_files[fname + ".idx"] = File(fname + ".idx")
    j.uses(tracked_files[fname + ".idx"], link=Link.OUTPUT, transfer=False)
    
    j.addArguments("-T", "GenotypeGVCFs",
                   "-R", extract_fasta_basefile(ref_files),
                   "-o", tracked_files[fname],
                   "-L", chromosome)
    for f in variant_files:
        j.addArguments("--variant", f)

    dax.addJob(j)
    

def combine_variants(dax, software_file, chromosomes, ref_files, tracked_files):

    add_executable(dax, "combine_variants", "gatk-wrapper")
    j = ComputeJob("combine_variants", cores = 1, mem_gb = 10,
                   partition = "part3")
    
    # inputs
    j.uses(software_file, link=Link.INPUT)
    for f in ref_files:
        j.uses(f, link=Link.INPUT)
    for chr in chromosomes:
        fname = conf.get("local", "run_id") + "-" + "GVCF_%s.vcf" % (chr)
        j.uses(tracked_files[fname], link=Link.INPUT)
        j.uses(tracked_files[fname + ".idx"], link=Link.INPUT)

    # outputs
    fname = conf.get("local", "run_id") + "-" + "All.vcf"
    tracked_files[fname] = File(fname)
    j.uses(tracked_files[fname], link=Link.OUTPUT, transfer=True)
    tracked_files[fname + ".idx"] = File(fname + ".idx")
    j.uses(tracked_files[fname + ".idx"], link=Link.OUTPUT, transfer=True)
    
    j.addArguments("-T", "CombineVariants",
                   "-R", extract_fasta_basefile(ref_files),
                   "-o", tracked_files[fname])
    for chr in chromosomes:
        fname = conf.get("local", "run_id") + "-" + "GVCF_%s.vcf" % (chr)
        j.addArguments("--variant", tracked_files[fname])
    
    dax.addJob(j)

    # filter the results    
    tracked_files[conf.get("local", "run_id") + "-" + 'All_filtered_snp.vcf'] = \
            File(conf.get("local", "run_id") + "-" + "All_filtered_snp.vcf")
    tracked_files[conf.get("local", "run_id") + "-" + 'All_filtered_snp.vcf.idx'] = \
            File(conf.get("local", "run_id") + "-" + "All_filtered_snp.vcf.idx")
    select_and_filter_snp(dax, software_file, ref_files, tracked_files,
                          tracked_files[conf.get("local", "run_id") + "-" + 'All.vcf'],
                          tracked_files[conf.get("local", "run_id") + "-" + 'All_filtered_snp.vcf'],
                          tracked_files[conf.get("local", "run_id") + "-" + 'All_filtered_snp.vcf.idx'])
    
    tracked_files[conf.get("local", "run_id") + "-" + 'All_filtered_indel.vcf'] = \
            File(conf.get("local", "run_id") + "-" + "All_filtered_indel.vcf")
    tracked_files[conf.get("local", "run_id") + "-" + 'All_filtered_indel.vcf.idx'] = \
            File(conf.get("local", "run_id") + "-" + "All_filtered_indel.vcf.idx")
    select_and_filter_indel(dax, software_file, ref_files, tracked_files,
                            tracked_files[conf.get("local", "run_id") + "-" + 'All.vcf'],
                            tracked_files[conf.get("local", "run_id") + "-" + 'All_filtered_indel.vcf'],
                            tracked_files[conf.get("local", "run_id") + "-" + 'All_filtered_indel.vcf.idx'])


def generate_dax():
    """ generates the Pegasus DAX (directed acyclic graph - abstract XML)
    which is a description of a workflow """
    
    logger.info("Generating abstract workflow (DAX)")
    
    dax = AutoADAG("soykb")
    
    # The key to adding jobs to this workflow is the AutoADAG - it allows you
    # to add jobs with listed input and output files, and then the AutoADAG
    # will figure out the relationships between the jobs. There is no need
    # to list parent/child relationships, but you can do that if you feel it
    # makes the relationships more clear than just specifying the
    # inputs/outputs.
        
    # email notificiations for when the state of the workflow changes
    dax.invoke('all',  conf.get("local", "pegasus_bin") +
                       "/../share/pegasus/notification/email")
    
    ref_urls = []
    chromosomes = []
    fastq_urls = []
    read_input_lists(ref_urls, chromosomes, fastq_urls)

    # determine how many TACC compute nodes we need
    num_inputs_in_set = min(len(fastq_urls) / 2, 100)
    conf.set("exec_environment", "hosts_part1", str( (num_inputs_in_set // 16 + 1) * 4 ))
    conf.set("exec_environment", "hosts_part2", str( (num_inputs_in_set // 16 + 1) * 4 ))
    conf.set("exec_environment", "hosts_part3", str( 1 ))

    # we need to bring a copy of the software with us
    software_tar = File("software.tar.gz")
    software_tar.addPFN(PFN(local_pfn(conf.get("local", "work_dir") + "/software.tar.gz"), "local"))
    dax.addFile(software_tar)    
    add_executable(dax, "software-wrapper", "software-wrapper")
    software_job = ComputeJob("software-wrapper", cores=1, mem_gb=1)
    software_job.uses(software_tar, link=Link.INPUT)
    dax.addJob(software_job)

    # we need to track files across jobs
    tracked_files = {}
    sample_names = []
    
    # reference genome - add some jobs to prepare reference genome
    ref_files = []
    prepare_ref_genome(dax, software_job, software_tar, ref_urls[0], ref_files)

    lane_count = len(fastq_urls)
    if conf.get("main", "inputs-style") == "pair-end":
        lane_count = len(fastq_urls) / 2

    for lane in range(lane_count):

        # input files for this lane
        if conf.get("main", "inputs-style") == "single-end":
            lfn = extract_lfn(fastq_urls[lane])
            if re.search("\.gz$", lfn):
                f_gz = File(extract_lfn(fastq_urls[lane]))
                f_gz.addPFN(PFN(fastq_urls[lane], "irods_iplant"))
                dax.addFile(f_gz)
                fa_name = extract_lfn(fastq_urls[lane])
                fa_name = re.sub("\..*", ".fa", fa_name)
                f_fa = File(fa_name)
                gunzip_job(dax, software_tar, f_gz, f_fa, [software_job])
            else:
                f_fa = File(extract_lfn(fastq_urls[lane]))
                f_fa.addPFN(PFN(fastq_urls[lane], "irods_iplant"))
                dax.addFile(f_fa)
            tracked_files['fastq_input'] = f_fa
            sample_name = extract_sample_name(tracked_files['fastq_input'].name)
        else:
            tracked_files['paired_read1_fastq'] = File(extract_lfn(fastq_urls[lane * 2]))
            tracked_files['paired_read1_fastq'].addPFN(PFN(fastq_urls[lane * 2], "irods_iplant"))
            dax.addFile(tracked_files['paired_read1_fastq'])
    
            tracked_files['paired_read2_fastq'] = File(extract_lfn(fastq_urls[lane * 2 + 1]))
            tracked_files['paired_read2_fastq'].addPFN(PFN(fastq_urls[lane * 2 + 1], "irods_iplant"))
            dax.addFile(tracked_files['paired_read2_fastq'])
    
            sample_name = extract_sample_name(tracked_files['paired_read1_fastq'].name)

        # files we need to track
        tracked_files['sam'] = File(conf.get("local", "run_id") + "-" + sample_name + "_aligned_reads.sam")
        tracked_files['sorted_reads'] = File(conf.get("local", "run_id") + "-" + sample_name + "_sorted_reads.bam")
        tracked_files['sorted_index'] = File(conf.get("local", "run_id") + "-" + sample_name + "_sorted_reads.bai")
        tracked_files['deduped_reads'] = File(conf.get("local", "run_id") + "-" + sample_name + "_deduped_reads.bam")
        tracked_files['deduped_index'] = File(conf.get("local", "run_id") + "-" + sample_name + "_deduped_reads.bai")
        tracked_files['deduped_metrics'] = File(conf.get("local", "run_id") + "-" + sample_name + "_deduped.metrics")
        tracked_files['addrepl_reads'] = File(conf.get("local", "run_id") + "-" + sample_name + "_addrepl.bam")
        tracked_files['addrepl_index'] = File(conf.get("local", "run_id") + "-" + sample_name + "_addrepl.bai")
        tracked_files['intervals'] = File(conf.get("local", "run_id") + "-" + sample_name + "_intervals.list")
        tracked_files['indel_realigned_reads'] = File(conf.get("local", "run_id") + "-" + sample_name + "_indel_realigned.bam")
        tracked_files['indel_realigned_index'] = File(conf.get("local", "run_id") + "-" + sample_name + "_indel_realigned.bai")
    
        # Step 1 - dependent jobs are now added in the parent jobs
        align_job = alignment_to_reference(sample_name,
                                           dax,
                                           software_tar,
                                           chromosomes,
                                           ref_files,
                                           tracked_files)
        dax.depends(parent=software_job, child=align_job)

        # keep a list of samples for the GenotypeGVCFs call
        sample_names.append(sample_name)

    # combine all haplotype_caller outputs into one merged file for output
    merge_gvcf(dax, software_tar, chromosomes, ref_files, tracked_files, sample_names)
    
    # run genotype_gvcfs per chromosome
    for chr in chromosomes:
        genotype_gvcfs(dax, software_tar, ref_files, tracked_files,
                       sample_names, chr)
        
    combine_variants(dax, software_tar, chromosomes, ref_files, tracked_files)
 
    # write out the dax
    dax_file = open(conf.get("local", "work_dir") + "/soykb.dax", "w")
    dax.writeXML(dax_file)
    dax_file.close()


def main():
    global conf
    
    setup_logger(False)

    # Configure command line option parser
    prog_usage = "usage: workflow-generator [options]"
    parser = optparse.OptionParser(usage=prog_usage)

    parser.add_option("-e", "--exec-env", action = "store", dest = "exec_env",
                      help = "Handle for the target execution environment.")

    # Parse command line options
    (options, args) = parser.parse_args()
    if options.exec_env == None:
        logger.fatal("Please specify an execution environment with --exec-env")
        sys.exit(1)

    # read the config file and add those settings to the option object
    conf = ConfigParser.SafeConfigParser({'username': getpass.getuser()})
    r = conf.read([os.environ['HOME'] + "/.soybean-workflow.conf", \
                  "conf/main.conf", \
                  "conf/%s/site.conf" % options.exec_env])
    if len(r) != 3:
        logger.fatal("Unable to read configuration files for that environment")
        sys.exit(1)

    if conf.get("main", "inputs-style") != "single-end" and \
       conf.get("main", "inputs-style") != "double-end":
        logger.fatal("Valid choicses for the main/inputs-style configuration is" + \
                     " single-end or double-end")
        sys.exit(1)

    conf.set("local", "username", getpass.getuser())
    conf.set("local", "exec_env", options.exec_env)
    conf.set("local", "top_dir", os.path.dirname(os.path.realpath( __file__ )))

    # run id
    conf.set("local", "run_id", time.strftime("%Y%m%d-%H%M%S", time.gmtime()))
    
    # add the run id to the work dir
    conf.set("local", "work_dir", conf.get("local", "work_dir") + "/" + 
                                  conf.get("local", "run_id"))
    
    # local Pegasus environment
    pegasus_config = os.path.join("pegasus-config") + " --noeoln --bin"
    pegasus_bin_dir = subprocess.Popen(pegasus_config,
                                       stdout=subprocess.PIPE,
                                       shell=True).communicate()[0]
    conf.set("local", "pegasus_bin", pegasus_bin_dir)

    # check proxy before doing anything else
    proxy_check()
    
    # create a local work directory for the workflow
    logger.info("Setting up work directory at %s" \
                %(conf.get("local", "work_dir")))
    if os.path.exists(conf.get("local", "work_dir")):
        logger.fatal("Work directory already exists") 
        os.exit(1)
    os.makedirs(conf.get("local", "work_dir"))

    # tar up the software
    logger.info("Tarring up software directory to send with jobs")
    myexec("tar czf " + conf.get("local", "work_dir") + \
           "/software.tar.gz software")

    generate_site_catalog()

    # FIXME: what should we copy / keep in the top dir?
    myexec("cp conf/" + conf.get("local", "exec_env") + 
           "/transformations.catalog " + 
           conf.get("local", "work_dir") + "/transformations.catalog")
    myexec("cp conf/" + conf.get("local", "exec_env") + 
           "/replica.catalog " + 
           conf.get("local", "work_dir") + "/replica.catalog")

    generate_dax()

    # submit
    logger.info("Planning workflow...")
    os.chdir(conf.get("local", "work_dir"))
    cmd = "pegasus-plan" + \
          " --conf " + conf.get("local", "top_dir") + \
          "/conf/" + conf.get("local", "exec_env") + "/pegasus.conf" + \
          " --dir ." + \
          " --relative-dir wf-" + conf.get("local", "run_id") + \
          " --sites execution"
    
    if conf.get("exec_environment", "output_site") != "":
        cmd += " --output-site " + conf.get("exec_environment", "output_site")
              
    if conf.get("exec_environment", "staging_site") != "":
        cmd += " --staging " + conf.get("exec_environment", "staging_site")
          
    if conf.get("exec_environment", "job_clustering") != "":
        cmd += " --cluster " + conf.get("exec_environment", "job_clustering")
          
    cmd += " --dax soykb.dax"
    logger.info(cmd)
    myexec(cmd + " 2>&1 | tee pegasus-plan.out")


if __name__ == "__main__":
    main()

