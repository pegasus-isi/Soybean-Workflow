#!/usr/bin/env python

from AutoADAG import *
import ConfigParser
from Pegasus.DAX3 import *
import logging
import math
import optparse
import os
import re
import socket
import string
import subprocess
import sys
import time


# to setup python lib dir for importing Pegasus PYTHON DAX API
pegasus_config = os.path.join("pegasus-config") + " --noeoln --python"
lib_dir = subprocess.Popen(pegasus_config,
                           stdout=subprocess.PIPE,
                           shell=True).communicate()[0]
#Insert this directory in our search path
os.sys.path.insert(0, lib_dir)



# --- global variables ----------------------------------------------------------------

logger = logging.getLogger("my_logger")

# --- classes -------------------------------------------------------------------------

class ComputeJob(Job):
    """ A Pegasus DAX Job with extra information such as cpu and memory
    requirements, for both single and peagaus-mpi-cluster execution
    """

    def __init__(self, name, cores=1, mem_gb=2, partition="default"):
        Job.__init__(self, name=name)

        # label based clustering
        self.addProfile(Profile(Namespace.PEGASUS, 
                                key="label",
                                value=partition))
  
        # required for the Pegasus accounting
        self.addProfile(Profile(Namespace.PEGASUS,
                                key="cores",
                                value=str(cores)))
  
        # standard resource requirements for all jobs
        mem_mb = mem_gb * 1000
        self.addProfile(Profile(Namespace.CONDOR,
                                key="request_cpus",
                                value=str(cores)))
        self.addProfile(Profile(Namespace.PEGASUS,
                                key="pmc_request_cpus",
                                value=str(cores)))
        self.addProfile(Profile(Namespace.CONDOR,
                                key="request_memory",
                                value=str(mem_mb)))
        self.addProfile(Profile(Namespace.PEGASUS,
                                key="pmc_request_memory",
                                value=str(mem_mb)))


# --- functions -----------------------------------------------------------------------


def setup_logger(verbose):
    """ Use a console logger for all output to the user """

    # log to the console
    console = logging.StreamHandler()

    # default log level - make logger/console match
    logger.setLevel(logging.INFO)
    console.setLevel(logging.INFO)

    if verbose:
        logger.setLevel(logging.DEBUG)
        console.setLevel(logging.DEBUG)

    # formatter
    formatter = logging.Formatter("%(asctime)s %(levelname)7s:  %(message)s")
    console.setFormatter(formatter)
    logger.addHandler(console)
    logger.debug("Logger has been configured")


def myexec(cmd_line):
    """ Convenience function as we are shelling out a fair amount """
    
    sys.stdout.flush()
    p = subprocess.Popen(cmd_line + " 2>&1", shell=True)
    stdoutdata, stderrdata = p.communicate()
    r = p.returncode
    if r != 0:
        raise RuntimeError("Command '%s' failed with error code %s" \
                           % (cmd_line, r))


def generate_site_catalog(conf):
    """ Uses a templete file to produce the Pegasus site catalog """
    
    logger.info("Generating sites.catalog")
    inf = open("conf/" + conf.get("local", "exec_env") + 
               "/sites.catalog.template", 'r')
    template = string.Template(inf.read())
    inf.close()

    outf = open(conf.get("local", "work_dir") + "/sites.catalog", "w")
    outf.write(template.substitute(
                        home = os.path.expanduser('~'),
                        top_dir = conf.get("local", "top_dir"),
                        work_dir = conf.get("local", "work_dir"),
                        pegasus_bin = conf.get("local", "pegasus_bin"),
                        irods_bin = conf.get("local", "irods_bin")))
    outf.close()
   

def read_input_lists(conf, ref_urls, fastq_urls):
    """ The user provides a list of reference file URLs and pairs of fastq 
    URLs to be processed.
    """
    
    # first the reference
    inf = open(conf.get("local", "top_dir") + "/inputs-ref.txt", "r")
    for line in inf.readlines():
        line = line.rstrip('\n')
        ref_urls.append(line)
    inf.close()

    inf = open(conf.get("local", "top_dir") + "/inputs-fastq.txt", "r")
    for line in inf.readlines():
        line = line.rstrip('\n')
        fastq_urls.append(line)
    inf.close()

    # sanity checks
    if len(fastq_urls) == 0:
        logger.error("Did not find fastq files")
        sys.exit(1)
    if len(fastq_urls) % 2 != 0:
        logger.error("Found an uneven number of fastq files in input list")
        sys.exit(1)


def extract_lfn(url):
    """ determine a logical file name (basename) from a given URL """
    return re.sub(".*/", "", url)
  

def extract_fasta_basefile(file_list):
    """ find the base fasta file given a list of reference files """
    for f in file_list:
        if re.search(".fasta$", f.name):
            return f


def extract_sample_name(url):
    """ sample name is the fist part of the base name (for example:
    HN0001 from HN001_FCD1P1JACXX_L6_SZAIPI024836-36_1.fq)
    """
    name = re.sub(".*/", "", url)
    name = re.sub("_.*", "", name)
    return name


def generate_dax(conf):
    """ generates the Pegasus DAX (directed acyclic graph - abstract XML)
    which is a description of a workflow """
    
    logger.info("Generating abstract workflow (DAX)")
    
    # decide how many lanes to keep in a group, that is how many lanes
    # to pack into for example a pegasus-mpi-cluster job
    lanes_per_group = int(conf.get("exec_environment", "cores_per_node"))
    
    dax = AutoADAG("pipeline")
        
    # email notificiations for when the state of the workflow changes
    dax.invoke('all',  conf.get("local", "pegasus_bin") +
                       "/../share/pegasus/notification/email")
    add_executables(conf, dax)

    # we need to bring a copy of the software with us
    software_tar = File("software.tar.gz")
    software_tar.addPFN(PFN("gsiftp://" + socket.gethostname() + \
               conf.get("local", "work_dir") + "/software.tar.gz", "local"))
    dax.addFile(software_tar)    
    software_job = ComputeJob("software-wrapper", cores=1, mem_gb=1)
    software_job.uses(software_tar, link=Link.INPUT)
    dax.addJob(software_job)

    ref_urls = []
    fastq_urls = []
    read_input_lists(conf, ref_urls, fastq_urls)
   
    ref_files = []
    for url in ref_urls:
        f = File(extract_lfn(url))
        f.addPFN(PFN(url, "irods_iplant"))
        dax.addFile(f)
        # put these in a list so jobs can pick them up
        ref_files.append(f)

    for lane in range(len(fastq_urls) / 2):

        group_id = lane // lanes_per_group

        # input files for this lane   
        paired_read1_fastq_file = File(extract_lfn(fastq_urls[lane * 2]))
        paired_read1_fastq_file.addPFN(PFN(fastq_urls[lane * 2], "irods_iplant"))
        dax.addFile(paired_read1_fastq_file)

        paired_read2_fastq_file = File(extract_lfn(fastq_urls[lane * 2 + 1]))
        paired_read2_fastq_file.addPFN(PFN(fastq_urls[lane * 2 + 1], "irods_iplant"))
        dax.addFile(paired_read2_fastq_file)

        sample_name = extract_sample_name(paired_read1_fastq_file.name)

        # files we need to track
        sam_file = File(sample_name + "_aligned_reads.sam")
        sorted_reads_file = File(sample_name + "_sorted_reads.bam")
        sorted_index_file = File(sample_name + "_sorted_reads.bai")
        deduped_reads_file = File(sample_name + "_deduped_reads.bam")
        deduped_index_file = File(sample_name + "_deduped_reads.bai")
        deduped_metrics_file = File(sample_name + "_deduped.metrics")
        addrepl_reads_file = File(sample_name + "_addrepl.bam")
        addrepl_index_file = File(sample_name + "_addrepl.bai")
        intervals_file = File(sample_name + "_intervals.list")
        indel_realigned_reads_file = File(sample_name + "_indel_realigned.bam")
        indel_realigned_index_file = File(sample_name + "_indel_realigned.bai")
        bqsr_file = File(sample_name + "_bqsr")
        plot_file = File(sample_name + "_plot")
        recalib_bqsr_file = File(sample_name + "_reclib_bqsr")
        recalib_plot_file = File(sample_name + "_reclib_plot")
        print_reads_file = File(sample_name + "_print_reads.bam")
        print_index_file = File(sample_name + "_print_reads.bai")
        reduced_reads_file = File(sample_name + "_reduced_reads,bam")
        reduced_index_file = File(sample_name + "_reduced_reads,bai")
        unified_genotyper_reads_file = File(sample_name + "_unified_genotyper.bam")
        unified_genotyper_index_file = File(sample_name + "_unified_genotyper.bai")
        haplotype_caller_reads_file = File(sample_name + "_haplotype_caller.bam")
        haplotype_caller_index_file = File(sample_name + "_haplotype_caller.bai")
    
        # Step 1
        align_job = alignment_to_reference(conf,
                                           software_tar,
                                           ref_files,
                                           paired_read1_fastq_file,
                                           paired_read2_fastq_file,
                                           sam_file)
        dax.addJob(align_job)

        # Step 2
        sortsam_job = sort_sam(conf,
                               software_tar,
                               sam_file,
                               sorted_reads_file,
                               sorted_index_file)
        dax.addJob(sortsam_job)
        
        # Step 3
        dedup_job = dedup(conf,
                          software_tar,
                          sorted_reads_file,
                          sorted_index_file,
                          deduped_reads_file,
                          deduped_index_file,
                          deduped_metrics_file)
        dax.addJob(dedup_job)

        # Step 4
        add_repl_job = add_replace(conf,
                                   software_tar,
                                   sample_name,
                                   deduped_reads_file,
                                   deduped_index_file,
                                   addrepl_reads_file,
                                   addrepl_index_file)
        dax.addJob(add_repl_job)

        # Step 5
        realign_job = realign_target_creator(conf,
                                             software_tar,
                                             ref_files,
                                             addrepl_reads_file,
                                             addrepl_index_file,
                                             intervals_file)
        dax.addJob(realign_job)

        # Step 6
        indel_realign_job = indel_realign(conf,
                                          software_tar,
                                          ref_files, 
                                          addrepl_reads_file,
                                          addrepl_index_file,
                                          intervals_file,
                                          indel_realigned_reads_file,
                                          indel_realigned_index_file)
        dax.addJob(indel_realign_job)

        # Step 7
        base_calib_job = base_recalibrator(conf,
                                           software_tar,
                                           ref_files,
                                           indel_realigned_reads_file,
                                           indel_realigned_index_file, 
                                           intervals_file,
                                           bqsr_file,
                                           plot_file)
        dax.addJob(base_calib_job)

        # Step 8
        base_calib_2_job = base_recalibrator_2(conf,
                                               software_tar,
                                               ref_files,
                                               indel_realigned_reads_file,
                                               indel_realigned_index_file,
                                               bqsr_file,
                                               recalib_bqsr_file, 
                                               recalib_plot_file)
        dax.addJob(base_calib_2_job)

        # Step 9
        print_reads_job = print_reads(conf,
                                      software_tar,
                                      ref_files,
                                      indel_realigned_reads_file,
                                      indel_realigned_index_file,
                                      bqsr_file,
                                      print_reads_file,
                                      print_index_file)
        dax.addJob(print_reads_job)

        # Step 10
        reduce_reads_job = reduce_reads(conf,
                                        software_tar,
                                        ref_files,
                                        print_reads_file,
                                        print_index_file,
                                        reduced_reads_file,
                                        reduced_index_file)
        dax.addJob(reduce_reads_job)

        # Step 11
        unified_genotyper_job = unified_genotyper(conf,
                                                  software_tar,
                                                  ref_files,
                                                  reduced_reads_file,
                                                  reduced_index_file,
                                                  unified_genotyper_reads_file,
                                                  unified_genotyper_index_file)
        dax.addJob(unified_genotyper_job)
   
        haplotype_caller_job = haplotype_caller(conf, 
                                                software_tar,
                                                ref_files,
                                                reduced_reads_file,
                                                reduced_index_file,
                                                haplotype_caller_reads_file,
                                                haplotype_caller_index_file)
        dax.addJob(haplotype_caller_job)
        
        
    # write out the dax
    dax_file = open(conf.get("local", "work_dir") + "/pipeline.dax", "w")
    dax.writeXML(dax_file)
    dax_file.close()


def add_executables(conf, dax):
    """ adds executables to the DAX-level replica catalog """


    base_url = "gsiftp://" + socket.gethostname() + \
               conf.get("local", "top_dir")    
    
    bwa_wrapper = Executable(name="bwa-wrapper", 
                             arch="x86_64",
                             installed=False)
    bwa_wrapper.addPFN(PFN(base_url + "/wrappers/bwa-wrapper", "local"))
    dax.addExecutable(bwa_wrapper)
    
    picard_wrapper = Executable(name="picard-wrapper",
                                arch="x86_64",
                                installed=False)
    picard_wrapper.addPFN(PFN(base_url + "/wrappers/picard-wrapper", "local"))
    dax.addExecutable(picard_wrapper)
    
    gatk_wrapper = Executable(name="gatk-wrapper", 
                              arch="x86_64",
                              installed=False)
    gatk_wrapper.addPFN(PFN(base_url + "/wrappers/gatk-wrapper", "local"))
    dax.addExecutable(gatk_wrapper)
    
    # software setup job
    software_wrapper = Executable(name="software-wrapper",
                                  arch="x86_64",
                                  installed=False)
    software_wrapper.addPFN(PFN(base_url + "/wrappers/software-wrapper", 
                                "local"))
    dax.addExecutable(software_wrapper)
    

def alignment_to_reference(conf, software_file, ref_files,
                           fastq1, fastq2, sam):
    """step 1
    bwa mem -M Soybean_ref_genome.fasta $fastq[0] $fastq[1] > $sam
    """
    j = ComputeJob("bwa-wrapper", cores = 1, mem_gb = 4,
                   partition = "bwa")
    
    # determine which is the fasta file
    for f in ref_files:
        j.uses(f, link=Link.INPUT)
    j.uses(software_file, link=Link.INPUT)
    j.uses(fastq1, link=Link.INPUT)
    j.uses(fastq2, link=Link.INPUT)
    j.uses(sam, link=Link.OUTPUT)
    
    j.setStdout(sam)
    
    j.addArguments("mem",
                   "-M", extract_fasta_basefile(ref_files), 
                   fastq1, fastq2)
    return j


def sort_sam(conf, software_file, sam, sorted_sam, sorted_index):
    """step 2
    java -Xmx22g -jar picard-tools-1.92/SortSam.jar CREATE_INDEX=TRUE
    MAX_RECORDS_IN_RAM=5000000 TMP_DIR= /home/skhan/tmp/ I=$hash_files{'Sam'}
    O=$hash_files{'Sorted_sam'} SO=coordinate VALIDATION_STRINGENCY=LENIENT;
    """
   
    j = ComputeJob("picard-wrapper", cores = 1, mem_gb = 22, 
                   partition = "largemem")
    j.uses(software_file, link=Link.INPUT)
    j.uses(sam, link=Link.INPUT)
    j.uses(sorted_sam, link=Link.OUTPUT)
    j.uses(sorted_index, link=Link.OUTPUT)
    
    j.addArguments("SortSam.jar",
                   "CREATE_INDEX=TRUE",
                   "MAX_RECORDS_IN_RAM=5000000",
                   "TMP_DIR=.",
                   "I=" + sam.name,
                   "O=" + sorted_sam.name,
                   "SO=coordinate",
                   "VALIDATION_STRINGENCY=LENIENT")
    return j


def dedup(conf, software_file, sorted_reads, sorted_index,
          deduped_reads, deduped_index, deduped_metrics):
    """ step 3
    java -Xmx22g -jar picard-tools-1.92/MarkDuplicates.jar CREATE_INDEX=TRUE
    MAX_RECORDS_IN_RAM=5000000 TMP_DIR=/home/skhan/tmp/
    I=$hash_files{'Sorted_sam'} O=$hash_files{'MarkDup'} METRICS_FILE=Duplicates_.$gtype
    VALIDATION_STRINGENCY=LENIENT;
    """
   
    j = ComputeJob("picard-wrapper", cores = 1, mem_gb = 22,
                   partition = "largemem")
    j.uses(software_file, link=Link.INPUT)
    j.uses(sorted_reads, link=Link.INPUT)
    j.uses(sorted_index, link=Link.INPUT)
    j.uses(deduped_reads, link=Link.OUTPUT)
    j.uses(deduped_index, link=Link.OUTPUT)
    j.uses(deduped_metrics, link=Link.OUTPUT)
    
    j.addArguments("MarkDuplicates.jar",
                   "CREATE_INDEX=TRUE",
                   "MAX_RECORDS_IN_RAM=5000000",
                   "TMP_DIR=.",
                   "I=" + sorted_reads.name,
                   "O=" + deduped_reads.name,
                   "METRICS_FILE=" + deduped_metrics.name,
                   "VALIDATION_STRINGENCY=LENIENT")
    return j


def add_replace(conf, software_file, sample_name, 
                deduped_reads, deduped_index, 
                addrepl_reads, addrepl_index):
    """ step 4
    java -Xmx22g -jar picard-tools-1.92/AddOrReplaceReadGroups.jar
    MAX_RECORDS_IN_RAM=5000000 TMP_DIR=/home/skhan/tmp/ I=$hash_files{'MarkDup'}
    O=$hash_files{'AddOrRep'} RGID=$gtype LB=$gtype PL=\"Illumina\" SM=$gtype CN=BGI
    RGPU=$gtype VALIDATION_STRINGENCY= LENIENT SORT_ORDER=coordinate
    CREATE_INDEX=TRUE;
    """
   
    j = ComputeJob("picard-wrapper", cores = 1, mem_gb = 22,
                   partition = "largemem")
    j.uses(software_file, link=Link.INPUT)
    j.uses(deduped_reads, link=Link.INPUT)
    j.uses(deduped_index, link=Link.INPUT)
    j.uses(addrepl_reads, link=Link.OUTPUT)
    j.uses(addrepl_index, link=Link.OUTPUT)
    
    j.addArguments("AddOrReplaceReadGroups.jar",
                   "MAX_RECORDS_IN_RAM=5000000",
                   "TMP_DIR=.",
                   "I=" + deduped_reads.name,
                   "O=" + addrepl_reads.name,
                   "RGID=" + sample_name,
                   "LB=" + sample_name,
                   "PL=\"Illumina\"",
                   "SM=" + sample_name,
                   "CN=BGI",
                   "RGPU=" + sample_name,
                   "VALIDATION_STRINGENCY=LENIENT",
                   "SORT_ORDER=coordinate",
                   "CREATE_INDEX=TRUE")
    return j


def realign_target_creator(conf, software_file, ref_files,
                           addrepl_reads, addrepl_index, intervals):
    """ step 5
    java -Xmx10g -jar GATK/GenomeAnalysisTK.jar -T RealignerTargetCreator -R
    Soybean_ref_genome.fasta -I $hash_files{'AddOrRep'} -o $hash_files{'intervals'} --
    fix_misencoded_quality_scores

    """
   
    j = ComputeJob("gatk-wrapper", cores = 1, mem_gb = 10,
                   partition = "largemem")
    j.uses(software_file, link=Link.INPUT)
    for f in ref_files:
        j.uses(f, link=Link.INPUT)
    j.uses(addrepl_reads, link=Link.INPUT)
    j.uses(addrepl_index, link=Link.INPUT)
    j.uses(intervals, link=Link.OUTPUT)
    
    j.addArguments("GenomeAnalysisTK.jar",
                   "-T", "RealignerTargetCreator",
                   "-R", extract_fasta_basefile(ref_files),
                   "-I", addrepl_reads,
                   "-o", intervals,
                   "--fix_misencoded_quality_scores")
    return j


def indel_realign(conf, software_file, ref_files,
                  addrepl_reads, addrepl_index, intervals,
                  indel_realigned_reads, indel_realigned_index):
    """ step 6
    java -Xmx10g -jar GATK/GenomeAnalysisTK.jar -T IndelRealigner -R
    Soybean_ref_genome.fasta -I $hash_files{'AddOrRep'} -targetIntervals $hash_files{'intervals'}
    -o $hash_files{'Indelrealign'} --fix_misencoded_quality_scores;
    """
   
    j = ComputeJob("gatk-wrapper", cores = 1, mem_gb = 10,
                   partition = "largemem")
    j.uses(software_file, link=Link.INPUT)
    for f in ref_files:
        j.uses(f, link=Link.INPUT)
    j.uses(addrepl_reads, link=Link.INPUT)
    j.uses(addrepl_index, link=Link.INPUT)
    j.uses(intervals, link=Link.INPUT)
    j.uses(indel_realigned_reads, link=Link.OUTPUT)
    j.uses(indel_realigned_index, link=Link.OUTPUT)
    
    j.addArguments("GenomeAnalysisTK.jar",
                   "-T", "IndelRealigner",
                   "-R", extract_fasta_basefile(ref_files),
                   "-I", addrepl_reads,
                   "-targetIntervals", intervals,
                   "-o", indel_realigned_reads,
                   "--fix_misencoded_quality_scores")
    return j


def base_recalibrator(conf, software_file, ref_files,
                      indel_realigned_reads, indel_realigned_index,
                      intervals, bqsr, plot):
    """ step 7
    java -Xmx10g -jar GATK/GenomeAnalysisTK.jar -T BaseRecalibrator -I
    $hash_files{'Indelrealign'} -R Soybean_ref_genome.fasta -
    run_without_dbsnp_potentially_ruining_quality -o $hash_files{'bqsr'} -plots
    $hash_files{'plot'};
    """
   
    j = ComputeJob("gatk-wrapper", cores = 1, mem_gb = 10,
                   partition = "largemem")
    j.uses(software_file, link=Link.INPUT)
    for f in ref_files:
        j.uses(f, link=Link.INPUT)
    j.uses(indel_realigned_reads, link=Link.INPUT)
    j.uses(indel_realigned_index, link=Link.INPUT)
    j.uses(intervals, link=Link.INPUT)
    j.uses(bqsr, link=Link.OUTPUT)
    j.uses(plot, link=Link.OUTPUT)
    
    j.addArguments("GenomeAnalysisTK.jar",
                   "-T", "BaseRecalibrator",
                   "-l", "DEBUG",
                   "-R", extract_fasta_basefile(ref_files),
                   "-I", indel_realigned_reads,
                   "-run_without_dbsnp_potentially_ruining_quality",
                   "-o", bqsr,
                   "-plots", plot)
    return j


def base_recalibrator_2(conf, software_file, ref_files,
                        indel_realigned_reads, indel_realigned_index,
                        bqsr, recalib_bqsr, recalib_plot):
    """ step 8
    java -Xmx10g -jar GATK/GenomeAnalysisTK.jar -T BaseRecalibrator -I
    $hash_files{'Indelrealign'} -R Soybean_ref_genome.fasta -
    run_without_dbsnp_potentially_ruining_quality -BQSR $hash_files{'bqsr'} -o
    $hash_files{'recalib_bqsr'} -plots $hash_files{'recalib_plot'};
    """
   
    j = ComputeJob("gatk-wrapper", cores = 1, mem_gb = 10,
                   partition = "largemem")
    j.uses(software_file, link=Link.INPUT)
    for f in ref_files:
        j.uses(f, link=Link.INPUT)
    j.uses(indel_realigned_reads, link=Link.INPUT)
    j.uses(indel_realigned_index, link=Link.INPUT)
    j.uses(bqsr, link=Link.INPUT)
    j.uses(recalib_bqsr, link=Link.OUTPUT)
    j.uses(recalib_plot, link=Link.OUTPUT)
    
    j.addArguments("GenomeAnalysisTK.jar",
                   "-T", "BaseRecalibrator",
                   "-R", extract_fasta_basefile(ref_files),
                   "-I", indel_realigned_reads,
                   "-run_without_dbsnp_potentially_ruining_quality",
                   "-BQSR", bqsr,
                   "-o", recalib_bqsr,
                   "-plots", recalib_plot)
    return j


def print_reads(conf, software_file, ref_files,
                indel_realigned_reads, indel_realigned_index, bqsr,
                print_reads_file, print_index_file):
    """ step 9
    java -Xmx10g -jar GATK/GenomeAnalysisTK.jar -T PrintReads -R Soybean_ref_genome.fasta -I
    $hash_files{'Indelrealign'} -BQSR $hash_files{'bqsr'} -o $hash_files{'PrintReads'};
    """
   
    j = ComputeJob("gatk-wrapper", cores = 1, mem_gb = 10,
                   partition = "largemem")
    j.uses(software_file, link=Link.INPUT)
    for f in ref_files:
        j.uses(f, link=Link.INPUT)
    j.uses(indel_realigned_reads, link=Link.INPUT)
    j.uses(indel_realigned_index, link=Link.INPUT)
    j.uses(bqsr, link=Link.INPUT)
    j.uses(print_reads_file, link=Link.OUTPUT)
    j.uses(print_index_file, link=Link.OUTPUT)
    
    j.addArguments("GenomeAnalysisTK.jar",
                   "-T", "PrintReads",
                   "-R", extract_fasta_basefile(ref_files),
                   "-I", indel_realigned_reads,
                   "-BQSR", bqsr,
                   "-o", print_reads_file)
    return j


def reduce_reads(conf, software_file, ref_files,
                 print_reads_file, print_index_file,
                 reduced_reads_file, reduced_index_file):
    """ step 10
    java -Xmx10g -Djava.io.tmpdir=/home/skhan/tmp -jar GATK/GenomeAnalysisTK.jar -T
    ReduceReads -R Soybean_ref_genome.fasta -I $hash_files{'PrintReads'} -o
    $hash_files{'ReduceReads'};
    """
   
    j = ComputeJob("gatk-wrapper", cores = 1, mem_gb = 10,
                   partition = "largemem")
    j.uses(software_file, link=Link.INPUT)
    for f in ref_files:
        j.uses(f, link=Link.INPUT)
    j.uses(print_reads_file, link=Link.INPUT)
    j.uses(print_index_file, link=Link.INPUT)
    j.uses(reduced_reads_file,link=Link.OUTPUT)
    j.uses(reduced_index_file,link=Link.OUTPUT)
    
    j.addArguments("GenomeAnalysisTK.jar",
                   "-T", "ReduceReads",
                   "-R", extract_fasta_basefile(ref_files),
                   "-I", print_reads_file,
                   "-o", reduced_reads_file)
    return j


def unified_genotyper(conf, software_file, ref_files,
                      reduced_reads_file, reduced_index_file,
                      unified_genotyper_reads, unified_genotype_index):
    """ step 11
    java -Xmx10g -Djava.io.tmpdir=/home/skhan/tmp -jar GATK/GenomeAnalysisTK.jar -T
    UnifiedGenotyper -R Soybean_ref_genome.fasta -I $hash_files{'ReduceReads'} -o
    $hash_files{'UnifiedGenotyper'};
    """
   
    j = ComputeJob("gatk-wrapper", cores = 1, mem_gb = 10,
                   partition = "largemem")
    j.uses(software_file, link=Link.INPUT)
    for f in ref_files:
        j.uses(f, link=Link.INPUT)
    j.uses(reduced_reads_file, link=Link.INPUT)
    j.uses(unified_genotyper_reads,link=Link.OUTPUT)
    
    j.addArguments("GenomeAnalysisTK.jar",
                   "-T", "UnifiedGenotyper",
                   "-R", extract_fasta_basefile(ref_files),
                   "-I", reduced_reads_file,
                   "-o", unified_genotyper_reads)
    return j


def haplotype_caller(conf, software_file, ref_files,
                    reduced_reads, reduced_index,
                    haplotype_caller_reads, haplotype_caller_index):
    """
    java -Xmx10g -Djava.io.tmpdir=/home/skhan/tmp -jar GATK/GenomeAnalysisTK.jar
    -T HaplotypeCaller -R Soybean_ref_genome.fasta -I $hash_files{'ReduceReads'}
    -o $hash_files{'HaplotypeCaller '}
    """
   
    j = ComputeJob("gatk-wrapper", cores = 1, mem_gb = 10,
                   partition = "largemem")
    j.uses(software_file, link=Link.INPUT)
    for f in ref_files:
        j.uses(f, link=Link.INPUT)
    j.uses(reduced_reads, link=Link.INPUT)
    j.uses(reduced_index, link=Link.INPUT)
    j.uses(haplotype_caller_reads,link=Link.OUTPUT)
    j.uses(haplotype_caller_index,link=Link.OUTPUT)
    
    j.addArguments("GenomeAnalysisTK.jar",
                   "-T", "HaplotypeCaller",
                   "-R", extract_fasta_basefile(ref_files),
                   "-I", reduced_reads,
                   "-o", haplotype_caller_reads)
    return j



def main():
    
    setup_logger(False)

    # Configure command line option parser
    prog_usage = "usage: workflow-generator [options]"
    parser = optparse.OptionParser(usage=prog_usage)

    parser.add_option("-e", "--exec-env", action = "store", dest = "exec_env",
                      help = "Handle for the target execution environment.")

    # Parse command line options
    (options, args) = parser.parse_args()
    if options.exec_env == None:
        logger.fatal("Please specify an execution environment with --exec-env")
        sys.exit(1)

    # read the config file and add those settings to the option object
    conf = ConfigParser.ConfigParser()
    r = conf.read(["conf/local.conf", "conf/%s/site.conf" % options.exec_env])
    if len(r) != 2:
        logger.fatal("Unable to read configuration files for that environment")
        sys.exit(1)

    conf.set("local", "exec_env", options.exec_env)
    conf.set("local", "top_dir", os.path.dirname(os.path.realpath( __file__ )))

    # run id
    conf.set("local", "run_id", time.strftime("%Y%m%d-%H%M%S", time.gmtime()))

    # add the run id to the work dir
    conf.set("local", "work_dir", conf.get("local", "work_dir") + "/" + 
                                  conf.get("local", "run_id"))
    
    # local Pegasus environment
    pegasus_config = os.path.join("pegasus-config") + " --noeoln --bin"
    pegasus_bin_dir = subprocess.Popen(pegasus_config,
                                       stdout=subprocess.PIPE,
                                       shell=True).communicate()[0]
    conf.set("local", "pegasus_bin", pegasus_bin_dir)
    
    # create a local work directory for the workflow
    logger.info("Setting up work directory at %s" \
                %(conf.get("local", "work_dir")))
    if os.path.exists(conf.get("local", "work_dir")):
        logger.fatal("Work directory already exists") 
        os.exit(1)
    os.makedirs(conf.get("local", "work_dir"))

    # tar up the software
    logger.info("Tarring up software directory to send with jobs")
    myexec("tar czf " + conf.get("local", "work_dir") + \
           "/software.tar.gz software")

    generate_site_catalog(conf)

    # FIXME: what should we copy / keep in the top dir?
    myexec("cp conf/" + conf.get("local", "exec_env") + 
           "/transformations.catalog " + 
           conf.get("local", "work_dir") + "/transformations.catalog")
    myexec("cp conf/" + conf.get("local", "exec_env") + 
           "/replica.catalog " + 
           conf.get("local", "work_dir") + "/replica.catalog")

    generate_dax(conf)

    # submit
    logger.info("Planning workflow...")
    os.chdir(conf.get("local", "work_dir"))
    cmd = "pegasus-plan" + \
          " --conf " + conf.get("local", "top_dir") + \
          "/conf/" + conf.get("local", "exec_env") + "/pegasus.conf" + \
          " --dir ." + \
          " --nocleanup" + \
          " --sites execution" + \
          " --output-site local"
              
    if conf.get("exec_environment", "staging_site") != "":
        cmd += " --staging " + conf.get("exec_environment", "staging_site")
          
    if conf.get("exec_environment", "job_clustering") != "":
        cmd += " --cluster " + conf.get("exec_environment", "job_clustering")
          
    cmd += " --dax pipeline.dax"              
    logger.info(cmd)
    myexec(cmd + " 2>&1 | tee pegasus-plan.out")


if __name__ == "__main__":
    main()

